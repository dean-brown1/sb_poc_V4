dataset:
  name: mbpp
  num_train_samples: null
  split_test: test
  split_train: train
  tagging_method: hash
evaluation:
  do_sample: false
  eval_interval: 50
  max_new_tokens: 512
  num_test_samples: 500
experiment:
  mode: schemabank
  name: schemabank_mbpp_8epochs
  output_dir: ./results/schemabank_mbpp_8epochs
  seed: 42
logging:
  log_every_n_steps: 1
  save_final_only: true
lora:
  bias: none
  lora_alpha: 16
  lora_dropout: 0.05
  r: 8
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  task_type: CAUSAL_LM
model:
  base_model: Qwen/Qwen2-0.5B
  device_map: auto
  torch_dtype: bfloat16
schemabank:
  attr_dim: 32
  layers: last_2
  num_schemas: 32
  rank: 16
  regularization:
    ortho_weight: 0.0001
  topk: 2
training:
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 0.0001
  max_grad_norm: 1.0
  seq_len: 1024
  stages:
    stage1_router_pretrain:
      description: Router learns schema mapping
      frozen:
      - base_model
      - lora_adapters
      - schema_transforms
      steps: 748
      tag_curriculum:
        dropout_schedule:
        - 0.0
        - 0.25
        - 0.5
        - 0.75
      trainable:
      - router_weights
    stage2_schema_train:
      description: Schemas learn transformations
      frozen:
      - base_model
      - router_weights
      steps: 1496
      trainable:
      - schema_transforms
      - lora_adapters
      use_tags: false
    stage3_joint_finetune:
      description: Joint optimization
      frozen:
      - base_model
      steps: 748
      trainable:
      - router_weights
      - schema_transforms
      - lora_adapters
      use_tags: false
  total_steps: 2992
  warmup_ratio: 0.05
  weight_decay: 0.01
